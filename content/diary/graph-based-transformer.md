---
title: Exploring Embedded latent spaces with Graph based Transformer.
linktitle: "April 3, 2023: My introduction on Graph based Transformer"
date: 2023-04-03
# Prev/next pager order (if `docs_section_pager` enabled in `params.toml`)
weight: 1
---

After researching and gaining an understanding of the core concepts of GNN (Graph Neural Networks) in the field of traffic forecasting, I am trying to understand the latest research in the area. However, the field is currently under development, and sometimes finding a clear route can be a little difficult. The issue is that there is still no clear, current state-of-the-art architecture that can be used to properly study the latest top contributions. Besides that, there is a current trend in exploring the transformer architecture in the field. This seems like a natural movement, considering the fame these architectures have acquired in NLP and other developments. It appears that transformers are becoming more and more multi-purpose frameworks for almost all problems in deep learning. What drives me to explore these architectures is their potential for feature extraction and recognition. These models have the ability to efficiently encode and decode temporal and spatial information from datasets. Other researchers focus on interesting architectures like STGCN (Spatial Temporal Graph Neural Networks), which utilize GCN for spatial information and LSTM or GRU for predicting temporal information. I am not looking into this type of direction or exploring more combinations of Spatial Temporal Graph Neural Networks because it is a well-studied problem, and I have low confidence that I could continue improving it. Moreover, transformer-based architectures have demonstrated better performance in predicting outcomes, possibly due to the vanishing and exploding gradient effect problem. This problem occurs when, after some layers, the content of the initial outcomes seems to disappear from each temporal layer. This issue arises more from the temporal side rather than the spatial aspect. Methods like GAT (Graph Attention Networks) have already been widely used in the field and employ self-attention mechanisms. Additionally, the ability to create latent graph spaces, where objects within the sets are graphs, presents interesting ideas for further development.

### Transformers to Graphs:
The current architectures that I am going to explore are ASTGNN and ASTTN. Both inherit from the fundamental architecture of adapting transformers to a type of GNN. Some of the papers that explore this type of architecture include the following [1](https://arxiv.org/abs/2012.09699),[2](https://ieeexplore.ieee.org/document/9346058),[3](https://arxiv.org/abs/2207.05064). In addition to these readings, I recommend the following [blog](https://towardsdatascience.com/graph-transformer-generalization-of-transformers-to-graphs-ead2448cff8b) post to fully understand some of the ideas for applying this to graphs. I especially recommend understanding the key concept of positional embedding with the eigenvectors of the Laplacian Matrix. Also, there is slight confusion in the naming of some architectures like Graph Transformer Networks ([GTN](https://arxiv.org/abs/1911.06455)). Although it has the name "transformer," it is a method for creating embedded meta-paths for predictions. That being said, methods with heterogeneous graphs are a very interesting topic for me to explore and how they can adapt to the problem of traffic forecasting. Some of the most intriguing ideas that come to my mind involve generalizing the typical homogeneous graph approach to more generic graph node and edge feature learning.